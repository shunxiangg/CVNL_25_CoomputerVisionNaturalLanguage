{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3778e1e",
   "metadata": {},
   "source": [
    "# MLP in PyTorch - Architecture & Assumptions\n",
    "\n",
    "**Architecture:** Input(4) -> Hidden1(64, ReLU) -> Hidden2(32, ReLU) -> Output(3).\n",
    "\n",
    "**Loss:** Cross-entropy loss (nn.CrossEntropyLoss).\n",
    "\n",
    "**Training:** 5 epochs on dummy data (100 samples).\n",
    "\n",
    "**Evaluation metric:** Accuracy computed from softmax probabilities (for reporting).\n",
    "\n",
    "**Assumptions / Notes:**\n",
    "- We keep the model output as raw logits because `nn.CrossEntropyLoss` expects logits (it applies LogSoftmax internally).\n",
    "- For reporting / predictions we apply `softmax` to the logits to obtain class probabilities and then take `argmax` to get predicted labels.\n",
    "- The notebook uses randomly generated dummy data. Replace `inputs` and `targets` with your dataset when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Reproducibility (optional)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 1. Define the Architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # Input Layer (4) -> First Hidden Layer (64)\n",
    "        self.hidden1 = nn.Linear(4, 64)\n",
    "        # First Hidden (64) -> Second Hidden Layer (32)\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        # Second Hidden (32) -> Output Layer (3)\n",
    "        self.output = nn.Linear(32, 3)\n",
    "        \n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first layer and apply ReLU\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        # Pass through second layer and apply ReLU\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        # Output layer returns raw logits (CrossEntropyLoss expects logits)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 2. Initialize Model, Loss, and Optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()  # Combines LogSoftmax and NLLLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 3. Generate Dummy Data (Assumption)\n",
    "# 100 samples, 4 features each\n",
    "inputs = torch.randn(100, 4)\n",
    "# 100 target labels (classes 0, 1, or 2)\n",
    "targets = torch.randint(0, 3, (100,), dtype=torch.long)\n",
    "\n",
    "# 4. Training Loop (5 Epochs)\n",
    "num_epochs = 5\n",
    "softmax = nn.Softmax(dim=1)  # for reporting / predictions only\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Forward Pass ---\n",
    "    outputs = model(inputs)  # logits\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # --- Backward Pass & Optimization ---\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "\n",
    "    # --- Evaluation Metric: Accuracy ---\n",
    "    # Apply softmax to get probabilities (for reporting only)\n",
    "    probs = softmax(outputs)\n",
    "    _, predicted = torch.max(probs, 1)\n",
    "\n",
    "    # Calculate accuracy: mean of correct predictions\n",
    "    accuracy = (predicted == targets).float().mean().item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562897e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/5], Loss: 1.0848, Accuracy: 44.00%\n",
      "Epoch [2/5], Loss: 1.0557, Accuracy: 42.00%\n",
      "Epoch [3/5], Loss: 1.0332, Accuracy: 46.00%\n",
      "Epoch [4/5], Loss: 1.0116, Accuracy: 48.00%\n",
      "Epoch [5/5], Loss: 0.9938, Accuracy: 49.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Define the MLP Architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # Input Layer (4 features) -> First Hidden Layer (64 neurons)\n",
    "        self.hidden1 = nn.Linear(4, 64)\n",
    "        # First Hidden (64) -> Second Hidden Layer (32 neurons)\n",
    "        self.hidden2 = nn.Linear(64, 32)\n",
    "        # Second Hidden (32) -> Output Layer (3 neurons)\n",
    "        self.output = nn.Linear(32, 3)\n",
    "        \n",
    "        # Activation function defined once to be reused\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first layer and apply ReLU\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        # Pass through second layer and apply ReLU\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        # Pass through output layer\n",
    "        # Note: We return raw logits here because nn.CrossEntropyLoss \n",
    "        # applies Softmax internally.\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 2. Model Initialization\n",
    "model = MLP()\n",
    "\n",
    "# Loss Function: CrossEntropyLoss (Includes Softmax internally)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 3. Create Dummy Data (Assumption)\n",
    "# 100 samples, 4 features each\n",
    "inputs = torch.randn(100, 4)\n",
    "# 100 random target labels (classes 0, 1, or 2)\n",
    "targets = torch.randint(0, 3, (100,))\n",
    "\n",
    "# 4. Training Loop (5 Epochs)\n",
    "num_epochs = 5\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Forward Pass ---\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # --- Backward Pass & Optimization ---\n",
    "    optimizer.zero_grad()  # Reset gradients to zero\n",
    "    loss.backward()        # Compute gradients (backpropagation)\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    # --- Evaluation Metric: Accuracy ---\n",
    "    # torch.max returns (values, indices). We need indices for the class ID.\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Calculate accuracy: Correct Predictions / Total Samples\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    accuracy = correct / targets.size(0)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
