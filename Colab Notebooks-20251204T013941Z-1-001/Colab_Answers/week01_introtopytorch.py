# -*- coding: utf-8 -*-
"""Week01_IntroToPyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XsMhYqNxdRpp9EOVX3vaFJB45HJAt3T7
"""
# Check installed packages in the environment
!pip list
# Check the status of the NVIDIA GPU (if available).
# This is useful to verify if a GPU is allocated to your Colab session.
!nvidia-smi

"""This is beginnning of a small boss in cnvl."""
# Run nvidia-smi again (redundant here, but often used to monitor GPU usage)
!nvidia-smi

import torch
torch_scalar = torch.tensor(3.14)
torch_vector = torch.tensor([1, 2, 3, 4])
torch_matrix = torch.tensor([[1, 2,],
[3, 4,],
[5, 6,],
[7, 8,]])

torch_tensor3d = torch.tensor([
[
[ 1, 2, 3],
[ 4, 5, 6],
],
[
[ 7, 8, 9],
[10, 11, 12],
],
[
[13, 14, 15],
[16, 17, 18],
],
[
[19, 20, 21],
[22, 23, 24],
]
])

print(torch_scalar.shape)
print(torch_vector.shape)
print(torch_matrix.shape)
print(torch_tensor3d.shape)

import numpy as np
x_np = np.random.random((4,4))
print(x_np)

x_pt = torch.tensor(x_np)
print(x_pt)
print()
print(x_np.dtype, x_pt.dtype)

x_np = np.asarray(x_np, dtype=np.float32)
x_pt = torch.tensor(x_np, dtype=torch.float32)
print(x_np.dtype, x_pt.dtype)


#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================


# Check installed packages in the environment
!pip list

# Check the status of the NVIDIA GPU (if available).
# This is useful to verify if a GPU is allocated to your Colab session.
!nvidia-smi

"""This is beginnning of a small boss in cnvl."""

# Run nvidia-smi again (redundant here, but often used to monitor GPU usage)
!nvidia-smi

import torch

# --- CREATING TENSORS ---
# A 0-dimensional tensor (Scalar), represents a single number.
torch_scalar = torch.tensor(3.14)

# A 1-dimensional tensor (Vector).
torch_vector = torch.tensor([1, 2, 3, 4])

# A 2-dimensional tensor (Matrix). Shape is (4, 2).
torch_matrix = torch.tensor([[1, 2,],
                             [3, 4,],
                             [5, 6,],
                             [7, 8,]])

# A 3-dimensional tensor. Shape is (4, 2, 3).
# Structure: 4 blocks, each block has 2 rows, each row has 3 columns.
torch_tensor3d = torch.tensor([
    [
        [ 1, 2, 3],
        [ 4, 5, 6],
    ],
    [
        [ 7, 8, 9],
        [10, 11, 12],
    ],
    [
        [13, 14, 15],
        [16, 17, 18],
    ],
    [
        [19, 20, 21],
        [22, 23, 24],
    ]
])

# --- CHECKING SHAPES ---
# Prints the dimensions (size) of each tensor.
print(torch_scalar.shape)   # Output: torch.Size([])
print(torch_vector.shape)   # Output: torch.Size([4])
print(torch_matrix.shape)   # Output: torch.Size([4, 2])
print(torch_tensor3d.shape) # Output: torch.Size([4, 2, 3])

# --- NUMPY INTEROPERABILITY ---
import numpy as np

# Create a 4x4 matrix using NumPy with random values between 0 and 1.
# NumPy defaults to float64 data type.
x_np = np.random.random((4,4))
print(x_np)

# Convert the NumPy array directly to a PyTorch tensor.
# Note: The tensor inherits the data type (float64) from the NumPy array.
x_pt = torch.tensor(x_np)
print(x_pt)
print()

# Check and print the data types.
# NumPy default: float64. PyTorch from NumPy: float64 (Double).
print(x_np.dtype, x_pt.dtype)

# Explicitly cast the NumPy array to float32 (standard for Deep Learning).
x_np = np.asarray(x_np, dtype=np.float32)

# Convert the float32 NumPy array to a PyTorch tensor, explicitly specifying float32.
x_pt = torch.tensor(x_np, dtype=torch.float32)

# Verify the new data types. Both should now be float32.
print(x_np.dtype, x_pt.dtype)