# -*- coding: utf-8 -*-
"""Week02_IntroToPyTorchII.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16pLRkWvyo3yX9dgPjc379ubIxnNBnUgN
"""

import torch
import timeit
x = torch.rand(2**11, 2**11)
time_cpu = timeit.timeit("x@x", globals=globals(), number=100)
time_cpu

print("Is CUDA available? :", torch.cuda.is_available())
device = torch.device("cuda")

x = x.to(device)
time_gpu = timeit.timeit("x@x", globals=globals(), number=100)
print(time_gpu)

x = torch.rand(128, 128).to(device)
y = torch.rand(128, 128).to(device)
y*x

x = torch.rand(128, 128).to(device)
y = torch.rand(128, 128).to(device)
y*x

#x.numpy()
x.cpu().numpy()

def moveTo(obj, device):
  '''
  obj: the python object to move to a device, or to move its contents to a device
  device: the compute device to move objects to
  '''
  if isinstance(obj, list):
    return [moveTo(x, device) for x in obj]
  elif isinstance(obj, tuple):
    return tuple(moveTo(list(obj), device))
  elif isinstance(obj, set):
    return set(moveTo(list(obj), device))
  elif isinstance(obj, dict):
    to_ret = dict()
    for key, value in obj.items():
      to_ret[moveTo(key, device)] = moveTo(value, device)
    return to_ret
  elif hasattr(obj, "to"):
    return obj.to(device)
  else:
    return obj

some_tensors = [torch.tensor(1), torch.tensor(2)]
print(some_tensors)
print(moveTo(some_tensors, device))

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import seaborn as sns

def f(x):
  return torch.pow((x-2.0), 2)
x_axis_vals = np.linspace(-7,9,100)
y_axis_vals = f(torch.tensor(x_axis_vals)).numpy()
sns.lineplot(x=x_axis_vals, y=y_axis_vals, label='$f(x)=(x-2)^2$')

def fP(x): #Defines the derivative of f(x) manually
  return 2*x-4
y_axis_vals_p = fP(torch.tensor(x_axis_vals)).numpy()

#Draws a black line at 0 so we can easily tell if something is positive or
#negative
sns.lineplot(x=x_axis_vals, y=[0.0]*len(x_axis_vals), label="0", \
             color='black')
sns.lineplot(x=x_axis_vals, y=y_axis_vals, \
             label='Function to Minimize $f(x) = (x-2)^2$')
sns.lineplot(x=x_axis_vals, y=y_axis_vals_p, \
             label="Gradient of the function $f'(x)=2 x - 4$")

import torch
x = torch.tensor([-3.5], requires_grad=True)
print(x.grad)
None

value = f(x)
print(value)

value.backward()
print(x.grad)

x = torch.tensor([-3.5], requires_grad=True)
x_cur = x.clone()
x_prev = x_cur*100

#threshold for the current and previous to be close enough
#before stop.
epsilon = 1e-5
eta = 0.1 #learning rate
while torch.linalg.norm(x_cur-x_prev) > epsilon:
  x_prev = x_cur.clone() #so that x_prev and x_cur don't point to same object.
  value = f(x) #These next few lines computethe function, gradient, and update.

  value.backward()
  x.data -= eta * x.grad
  x.grad.zero_() #Zeros out the old gradient,

  x_cur = x.data #Accesses .data to avoid autograd mechanics.

print(x_cur)

x_param = torch.nn.Parameter(torch.tensor([-3.5]), requires_grad=True)

x = torch.tensor([-3.5], requires_grad=True)
optimizer = torch.optim.SGD([x_param], lr=eta)
for epoch in range(60):
  optimizer.zero_grad()
  loss_incurred = f(x_param)
  loss_incurred.backward()
  optimizer.step() #x.data -= eta * x.grad
print(x_param.data)

from torch.utils.data import Dataset
from sklearn.datasets import fetch_openml

#Loads data from
#https://www.openml.org/d/554
X, y = fetch_openml('mnist_784', version=1, return_X_y=True)

print(X.shape)

class SimpleDataset(Dataset):

  def __init__(self, X, y):
    super(SimpleDataset, self).__init__()
    # Convert X and y to numpy arrays
    self.X = X.to_numpy()
    self.y = y.to_numpy()

  def __getitem__(self, index):
    inputs = torch.tensor(self.X[index,:], dtype=torch.float32)
    targets = torch.tensor(int(self.y[index]), dtype=torch.int64)
    return inputs, targets

  def __len__(self):
    return self.X.shape[0]

dataset = SimpleDataset(X, y)

print("Length: ", len(dataset))
example, label = dataset[0]
print("Features: ", example.shape)
print("Label of index 0: ", label)