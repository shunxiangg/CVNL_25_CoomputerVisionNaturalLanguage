# -*- coding: utf-8 -*-
"""Week04_BetterTrainingCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9HtVszU16N-lzzU-8d28zCz-jijjPDB
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import *
from idlmam import *

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

from sklearn.datasets import make_moons

X_train, y_train = make_moons(n_samples=8000, noise=0.4)
X_test, y_test = make_moons(n_samples=200, noise=0.4)
train_dataset = TensorDataset(torch.tensor(X_train, \
                                           dtype=torch.float32), \
                              torch.tensor(y_train, dtype=torch.long))
test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                             torch.tensor(y_test, dtype=torch.long))
training_loader = DataLoader(train_dataset, shuffle=True)
testing_loader = DataLoader(test_dataset)

model = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)
loss_func = nn.CrossEntropyLoss()
device = torch.device("cuda")
results_pd = train_simple_network(model, loss_func, training_loader, \
                                  epochs=5, test_loader=testing_loader, \
                                  checkpoint_file='model.pt', \
                                  score_funcs={'Acc':accuracy_score,\
                                               'F1': f1_score})

def visualize2DSoftmax(X, y, model, title=None):
  x_min = np.min(X[:,0])-0.5
  x_max = np.max(X[:,0])+0.5
  y_min = np.min(X[:,1])-0.5
  y_max = np.max(X[:,1])+0.5

  xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20),\
                       np.linspace(y_min, y_max, num=20), indexing='ij')
  xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))

  with torch.no_grad():
    logits = model(torch.tensor(xy_v, dtype=torch.float32))
    y_hat = F.softmax(logits, dim=1).numpy()

  cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20),\
                    levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)
  ax = plt.gca()
  sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=ax)

  if title is not None:
    ax.set_title(title)

model_new = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)

visualize2DSoftmax(X_test, y_test, model_new, title="Initial Model")

plt.show()

checkpoint_dict = torch.load('model.pt', map_location=device)


model_new.load_state_dict(checkpoint_dict['model_state_dict'])

visualize2DSoftmax(X_test, y_test, model_new, title="Loaded Model")

plt.show()

sns.lineplot(x='epoch', y='train Acc', data=results_pd, label='Train')
sns.lineplot(x='epoch', y='test Acc', data=results_pd, label='Validation')

sns.lineplot(x='total time', y='train F1', data=results_pd, label='Train')
sns.lineplot(x='total time', y='test F1', data=results_pd, label='Validation')

training_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)
testing_loader = DataLoader(test_dataset, batch_size=len(test_dataset))
model_gd = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)
results_true_gd = train_simple_network(model_gd, loss_func, \
                                       training_loader, epochs=5, \
                                       test_loader=testing_loader, \
                                       checkpoint_file='model.pt', \
                                       score_funcs={'Acc':accuracy_score,\
                                                    'F1': f1_score})

sns.lineplot(x='total time', y='test Acc', data=results_pd, label='SGD, B=1')
sns.lineplot(x='total time', y='test Acc', data=results_true_gd, label='GD, B=N')

training_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
model_sgd = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)
results_batched = train_simple_network(model_sgd, loss_func, training_loader, epochs=5, test_loader=testing_loader, checkpoint_file='model.pt', score_funcs={'Acc':accuracy_score,\
                                                    'F1': f1_score})

sns.lineplot(x='total time', y='test Acc', data=results_pd, label='SGD, B=1')
sns.lineplot(x='total time', y='test Acc', data=results_true_gd, label='GD, B=N')
sns.lineplot(x='total time', y='test Acc', data=results_batched, label='SGD, B=32')


#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================

# -*- coding: utf-8 -*-
"""Week04_BetterTrainingCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I9HtVszU16N-lzzU-8d28zCz-jijjPDB
"""

# --- SETUP AND IMPORTS ---
from google.colab import drive
drive.mount('/content/drive')

import sys
# Adding the path to custom modules (like idlmam.py)
sys.path.append('/content/drive/My Drive/Colab Notebooks')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import *
from idlmam import * # Custom library used in the course

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

# Generate synthetic 'moons' dataset for classification
from sklearn.datasets import make_moons

# --- DATA PREPARATION ---
# Create 8000 training samples and 200 test samples. 
# noise=0.4 adds randomness to make the task harder.
X_train, y_train = make_moons(n_samples=8000, noise=0.4)
X_test, y_test = make_moons(n_samples=200, noise=0.4)

# Wrap data in PyTorch TensorDatasets
# Features (X) must be Float32, Labels (y) must be Long (int64) for classification
train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \
                              torch.tensor(y_train, dtype=torch.long))
test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                             torch.tensor(y_test, dtype=torch.long))

# Create DataLoaders. Default batch_size=1 (SGD) unless specified.
training_loader = DataLoader(train_dataset, shuffle=True)
testing_loader = DataLoader(test_dataset)

# --- MODEL DEFINITION ---
# A Multi-Layer Perceptron (MLP) with 2 input features (x, y coords)
# Hidden layers have 30 neurons. Tanh activation introduces non-linearity.
# Output has 2 neurons (one for each class).
model = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)

# Standard classification loss function (combines Softmax + Log Loss)
loss_func = nn.CrossEntropyLoss()
device = torch.device("cuda")

# --- TRAINING LOOP (First Run) ---
# Uses train_simple_network from idlmam module.
# Checkpoint saves the model state to 'model.pt' after every epoch.
# score_funcs allows tracking extra metrics like Accuracy and F1 Score.
results_pd = train_simple_network(model, loss_func, training_loader, \
                                  epochs=5, test_loader=testing_loader, \
                                  checkpoint_file='model.pt', \
                                  score_funcs={'Acc':accuracy_score,\
                                               'F1': f1_score})

# --- VISUALIZATION FUNCTION ---
# Plots the decision boundary of the model
def visualize2DSoftmax(X, y, model, title=None):
  # Define the boundaries of the plot based on data range
  x_min = np.min(X[:,0])-0.5
  x_max = np.max(X[:,0])+0.5
  y_min = np.min(X[:,1])-0.5
  y_max = np.max(X[:,1])+0.5

  # Create a meshgrid (grid of points) to evaluate the model everywhere
  xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20),\
                       np.linspace(y_min, y_max, num=20), indexing='ij')
  xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))

  # Evaluate model on the grid without gradients
  with torch.no_grad():
    logits = model(torch.tensor(xy_v, dtype=torch.float32))
    # Apply softmax to convert raw scores to probabilities
    y_hat = F.softmax(logits, dim=1).numpy()

  # Plot the decision boundary contours
  cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20),\
                    levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)
  ax = plt.gca()
  # Overlay the actual data points
  sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=ax)

  if title is not None:
    ax.set_title(title)

# --- LOADING AND TESTING MODELS ---
# Create a fresh, untrained model structure
model_new = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)

# Visualize untrained model (should be random/poor)
visualize2DSoftmax(X_test, y_test, model_new, title="Initial Model")
plt.show()

# Load the saved weights from the previous training run
checkpoint_dict = torch.load('model.pt', map_location=device)
model_new.load_state_dict(checkpoint_dict['model_state_dict'])

# Visualize loaded model (should match the trained performance)
visualize2DSoftmax(X_test, y_test, model_new, title="Loaded Model")
plt.show()

# --- PLOTTING TRAINING METRICS ---
# Plot Accuracy over epochs
sns.lineplot(x='epoch', y='train Acc', data=results_pd, label='Train')
sns.lineplot(x='epoch', y='test Acc', data=results_pd, label='Validation')

# Plot F1 Score over time
sns.lineplot(x='total time', y='train F1', data=results_pd, label='Train')
sns.lineplot(x='total time', y='test F1', data=results_pd, label='Validation')


# --- BATCH SIZE EXPERIMENTS ---

# EXPERIMENT 2: Gradient Descent (Batch Size = N)
# processing ALL data at once. Very stable gradient, but slow iteration.
training_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)
testing_loader = DataLoader(test_dataset, batch_size=len(test_dataset))

model_gd = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)
results_true_gd = train_simple_network(model_gd, loss_func, \
                                       training_loader, epochs=5, \
                                       test_loader=testing_loader, \
                                       checkpoint_file='model.pt', \
                                       score_funcs={'Acc':accuracy_score,\
                                                    'F1': f1_score})

# Compare SGD (Batch=1) vs GD (Batch=N)
sns.lineplot(x='total time', y='test Acc', data=results_pd, label='SGD, B=1')
sns.lineplot(x='total time', y='test Acc', data=results_true_gd, label='GD, B=N')

# EXPERIMENT 3: Mini-Batch Gradient Descent (Batch Size = 32)
# The "sweet spot". Updates often enough to learn fast, stable enough to converge.
training_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
model_sgd = nn.Sequential(
    nn.Linear(2,  30),
    nn.Tanh(),
    nn.Linear(30,  30),
    nn.Tanh(),
    nn.Linear(30, 2),
)
results_batched = train_simple_network(model_sgd, loss_func, training_loader, epochs=5, test_loader=testing_loader, checkpoint_file='model.pt', score_funcs={'Acc':accuracy_score,\
                                                    'F1': f1_score})

# Compare all three approaches
sns.lineplot(x='total time', y='test Acc', data=results_pd, label='SGD, B=1')
sns.lineplot(x='total time', y='test Acc', data=results_true_gd, label='GD, B=N')
sns.lineplot(x='total time', y='test Acc', data=results_batched, label='SGD, B=32')