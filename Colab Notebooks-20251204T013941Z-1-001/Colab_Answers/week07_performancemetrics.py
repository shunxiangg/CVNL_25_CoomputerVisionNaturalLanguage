# -*- coding: utf-8 -*-
"""Week07_PerformanceMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWMPZGZcpKQGoAqNSdhgnt2EOdoPgQiI
"""

# If needed, install packages (uncomment in a notebook)
# !pip install numpy scikit-learn torch

import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve, roc_auc_score,
    mean_squared_error, mean_absolute_error
)

import torch
import torch.nn as nn

# 1 = positive class, 0 = negative class

y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])
y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0])  # model predictions (labels)

acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred, pos_label=1)
rec = recall_score(y_true, y_pred, pos_label=1)
f1 = f1_score(y_true, y_pred, pos_label=1)

# Compute FPR explicitly from the confusion matrix
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
fpr = fp / (fp + tn)

print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1-score:", f1)
print("False Positive Rate (FPR):", fpr)

cm = confusion_matrix(y_true, y_pred)
print("Confusion matrix:\n", cm)

y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])
# model's probability that the class is 1
y_scores = np.array([0.9, 0.2, 0.8, 0.4, 0.3, 0.7, 0.95, 0.1])

fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)

print("FPR:", fpr)
print("TPR:", tpr)
print("Thresholds:", thresholds)

auc_value = roc_auc_score(y_true, y_scores)
print("AUC:", auc_value)

import matplotlib.pyplot as plt

plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')  # random baseline
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()

y_true_reg = np.array([3.0, 5.0, 2.5, 7.0])
y_pred_reg = np.array([2.5, 5.0, 4.0, 6.0])

mse = mean_squared_error(y_true_reg, y_pred_reg)
mae = mean_absolute_error(y_true_reg, y_pred_reg)
rmse = np.sqrt(mse)

print("MSE:", mse)
print("MAE:", mae)
print("RMSE:", rmse)

y_true_reg_t = torch.tensor([3.0, 5.0, 2.5, 7.0])
y_pred_reg_t = torch.tensor([2.5, 5.0, 4.0, 6.0])

mse_loss_fn = nn.MSELoss()
l1_loss_fn = nn.L1Loss()

mse_torch = mse_loss_fn(y_pred_reg_t, y_true_reg_t)
mae_torch = l1_loss_fn(y_pred_reg_t, y_true_reg_t)
rmse_torch = torch.sqrt(mse_torch)

print("PyTorch MSE:", mse_torch.item())
print("PyTorch MAE:", mae_torch.item())
print("PyTorch RMSE:", rmse_torch.item())

# Suppose we have 3 classes (0, 1, 2) and a batch of 3 samples
logits = torch.tensor([
    [2.0, 1.0, 0.1],  # sample 1 raw scores
    [0.5, 2.5, 0.3],  # sample 2
    [0.1, 0.2, 2.0]   # sample 3
], dtype=torch.float32)

targets = torch.tensor([0, 1, 2])  # true class indices

ce_loss_fn = nn.CrossEntropyLoss()
ce_loss = ce_loss_fn(logits, targets)

print("Cross-entropy loss (PyTorch):", ce_loss.item())


#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================

# -*- coding: utf-8 -*-
"""Week07_PerformanceMetrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWMPZGZcpKQGoAqNSdhgnt2EOdoPgQiI
"""

# If needed, install packages (uncomment in a notebook)
# !pip install numpy scikit-learn torch

import numpy as np
# Importing key metrics from sklearn
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_curve, roc_auc_score,
    mean_squared_error, mean_absolute_error
)

import torch
import torch.nn as nn

# --- CLASSIFICATION METRICS ---
# 1 = positive class, 0 = negative class
# Ground Truth labels
y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])
# Model Predictions (hard labels)
y_pred = np.array([1, 0, 1, 0, 0, 1, 1, 0]) 

# 1. Standard Metrics
acc = accuracy_score(y_true, y_pred) # (TP+TN)/Total
prec = precision_score(y_true, y_pred, pos_label=1) # TP/(TP+FP) - "How many selected items are relevant?"
rec = recall_score(y_true, y_pred, pos_label=1) # TP/(TP+FN) - "How many relevant items are selected?"
f1 = f1_score(y_true, y_pred, pos_label=1) # Harmonic mean of Precision and Recall

# 2. Confusion Matrix & False Positive Rate
# ravel() flattens the 2x2 matrix into 4 values: TN, FP, FN, TP
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
# FPR = FP / (FP + TN) -> Probability of false alarm
fpr = fp / (fp + tn)

print("Accuracy:", acc)
print("Precision:", prec)
print("Recall:", rec)
print("F1-score:", f1)
print("False Positive Rate (FPR):", fpr)

# Print Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion matrix:\n", cm)


# --- ROC & AUC (Receiver Operating Characteristic) ---
# ROC evaluates model performance at ALL classification thresholds.
# We need probability scores (not hard labels) for this.

y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])
# Model's probability output that the class is 1 (Positive)
y_scores = np.array([0.9, 0.2, 0.8, 0.4, 0.3, 0.7, 0.95, 0.1])

# Calculate ROC curve points (FPR, TPR) for various thresholds
fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)

print("FPR:", fpr)
print("TPR:", tpr)
print("Thresholds:", thresholds)

# AUC (Area Under Curve): Single number summary. 1.0 is perfect, 0.5 is random guessing.
auc_value = roc_auc_score(y_true, y_scores)
print("AUC:", auc_value)

# Plotting the ROC Curve
import matplotlib.pyplot as plt

plt.plot(fpr, tpr, marker='.')
plt.plot([0, 1], [0, 1], linestyle='--')  # Random baseline (diagonal)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.show()


# --- REGRESSION METRICS ---
# Evaluating continuous value predictions.

y_true_reg = np.array([3.0, 5.0, 2.5, 7.0])
y_pred_reg = np.array([2.5, 5.0, 4.0, 6.0])

# MSE (Mean Squared Error): Penalizes large errors heavily.
mse = mean_squared_error(y_true_reg, y_pred_reg)
# MAE (Mean Absolute Error): Average magnitude of errors.
mae = mean_absolute_error(y_true_reg, y_pred_reg)
# RMSE (Root Mean Squared Error): Same units as target variable.
rmse = np.sqrt(mse)

print("MSE:", mse)
print("MAE:", mae)
print("RMSE:", rmse)


# --- PYTORCH LOSS FUNCTIONS ---
# Replicating the above using PyTorch (useful for training loops).

y_true_reg_t = torch.tensor([3.0, 5.0, 2.5, 7.0])
y_pred_reg_t = torch.tensor([2.5, 5.0, 4.0, 6.0])

mse_loss_fn = nn.MSELoss()
l1_loss_fn = nn.L1Loss() # L1 Loss is equivalent to MAE

mse_torch = mse_loss_fn(y_pred_reg_t, y_true_reg_t)
mae_torch = l1_loss_fn(y_pred_reg_t, y_true_reg_t)
rmse_torch = torch.sqrt(mse_torch)

print("PyTorch MSE:", mse_torch.item())
print("PyTorch MAE:", mae_torch.item())
print("PyTorch RMSE:", rmse_torch.item())

# --- CROSS ENTROPY LOSS (Classification) ---
# Combines LogSoftmax and NLLLoss. Expects raw logits (scores), not probabilities.

# Batch of 3 samples, 3 classes (0, 1, 2)
logits = torch.tensor([
    [2.0, 1.0, 0.1],  # Sample 1: Class 0 has highest score
    [0.5, 2.5, 0.3],  # Sample 2: Class 1 has highest score
    [0.1, 0.2, 2.0]   # Sample 3: Class 2 has highest score
], dtype=torch.float32)

targets = torch.tensor([0, 1, 2])  # Correct class indices

ce_loss_fn = nn.CrossEntropyLoss()
ce_loss = ce_loss_fn(logits, targets)

# This single number is the average loss across the batch.
print("Cross-entropy loss (PyTorch):", ce_loss.item())