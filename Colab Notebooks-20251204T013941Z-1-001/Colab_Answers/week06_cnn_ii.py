# -*- coding: utf-8 -*-
"""Week06_CNN_II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LGILbXrpyRJDNpb0OF7Y-o5UUpaW3MK_
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks/Week06')

import torch
import torch.nn as nn
import torch.nn.functional as F


from torch.utils.data import Dataset, DataLoader

from tqdm.autonotebook import tqdm

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

import pandas as pd

from sklearn.metrics import accuracy_score

import time
from idlmam import *

import torchvision
from torchvision import transforms
from scipy.signal import convolve

mnist_data_train = torchvision.datasets.MNIST("./data", train=True,\
                                        download=True, \
                                        transform=transforms.ToTensor())

mnist_data_test = torchvision.datasets.MNIST("./data", train=False,\
                                   download=True, \
                                   transform=transforms.ToTensor())

x_example, y_example = mnist_data_train[0]

img_indx = 0
img, correct_class = mnist_data_train[img_indx]
img = img[0,:]
img_lr = np.roll(np.roll(img, 1, axis=1), 1, axis=0) #Moves to the lower right
img_ul = np.roll(np.roll(img, -1, axis=1), -1, axis=0) #then upper left

f, axarr = plt.subplots(1,3) #Plots the images
axarr[0].imshow(img, cmap='gray')
axarr[1].imshow(img_lr, cmap='gray')
axarr[2].imshow(img_ul, cmap='gray')
plt.show()

if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

B = 32
mnist_train_loader = DataLoader(mnist_data_train, batch_size=B, shuffle=True)
mnist_test_loader = DataLoader(mnist_data_test, batch_size=B)

#We use the number of values in the input
#to help determine the size of subsequent layers:
#28 * 28 images.
D = 28*28

#No of channels in input
C = 1

classes = 10

filters = 16

#Size of Kernel
K = 3

#For comparison, letâ€™s define a linear model
#of similar complexity.
model_linear = nn.Sequential(
  nn.Flatten(), # (B, C, W, H) -> (B, C*W*H) = (B,D)
  nn.Linear(D, 256),
  nn.Tanh(),
  nn.Linear(256, classes),
)

#Simple convolutional network.
#Conv2d follows the pattern
#Conv2d(No of input channels,
#filtersoutput-channels, filter-size).

model_cnn = nn.Sequential(
  nn.Conv2d(C, filters, K, padding=K//2), # x convolve with G
  #Activation functions
  #work on any size tensor.
  nn.Tanh(),
  #Converts from (B, C, W, H) ->(B, D)
  #so we can use a Linear layer.
  nn.Flatten(),
  nn.Linear(filters*D, classes),
)

loss_func = nn.CrossEntropyLoss()

cnn_results = train_simple_network(model_cnn, loss_func,\
                      mnist_train_loader, test_loader=mnist_test_loader,\
                        score_funcs={'Accuracy': accuracy_score}, \
                        device=device, epochs=20)

#eval mode since we are not training
model = model_cnn.cpu().eval()

def pred(model, img):
    with torch.no_grad():#Always turn off gradients when evaluating

        w, h = img.shape #Finds the width/height of the image
        if not isinstance(img, torch.Tensor):
            img = torch.tensor(img) #To make sure it works for both NumPy and PyTorch Tensor

        x = img.reshape(1,-1,w,h)#reshape it as (B, C, W, H)
        logits = model(x) #Get the logits
        y_hat = F.softmax(logits, dim=1)#Turn into probabilities
    return y_hat.numpy().flatten()#convert prediction to numpy array.

img_pred = pred(model, img)
img_lr_pred = pred(model, img_lr)
img_ul_pred = pred(model, img_ul)

print("Org Img Class {} Prob:         ".format(correct_class) , \
      img_pred[correct_class])
print("Lower Right Img Class {} Prob: ".format(correct_class) , \
      img_lr_pred[correct_class])
print("Upper Left Img Class {} Prob:   ".format(correct_class) , \
      img_ul_pred[correct_class])

model_cnn_pool = nn.Sequential(
  nn.Conv2d(C, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),
  nn.Conv2d(filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),

  nn.Flatten(),
  #Why did we reduce the number of units into the Linear layer by a factor
  #of 4 to power 2? Because pooling a 2x2 grid down to one value means we go from
  #4 values, down to 1, and we did this two times.
  nn.Linear(2*filters*D//(4**2), classes),
)

loss_func = nn.CrossEntropyLoss()

cnn_results_with_pool = train_simple_network(model_cnn_pool, loss_func, \
                                             mnist_train_loader, \
                                             test_loader=mnist_test_loader, \
                                             score_funcs={'Accuracy': accuracy_score}, \
                                             device=device, epochs=20)

model = model_cnn_pool.cpu().eval()
img_pred = pred(model, img)
img_lr_pred = pred(model, img_lr)
img_ul_pred = pred(model, img_ul)

print("Org Img Class {} Prob:         ".format(correct_class) , img_pred[correct_class])
print("Lower Right Img Class {} Prob: ".format(correct_class) , img_lr_pred[correct_class])
print("Uper Left Img Class {} Prob:   ".format(correct_class) , img_ul_pred[correct_class])

sns.lineplot(x='epoch', y='test Accuracy', data=cnn_results, label='Simple CNN')
sns.lineplot(x='epoch', y='test Accuracy', data=cnn_results_with_pool, label='CNN w/ Max Pooling')

#Built-in transformations, given some large values to make impact more obvious.
sample_transforms = {
    "Rotation" : transforms.RandomAffine(degrees=45),
    "Translation" : transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),
    "Shear": transforms.RandomAffine(degrees=0, shear=45),
    "RandomCrop" : transforms.RandomCrop((20,20)),
    "Horizontal Flip" : transforms.RandomHorizontalFlip(p=1.0),
    "Vertical Flip": transforms.RandomVerticalFlip(p=1.0),
    "Perspective": transforms.RandomPerspective(p=1.0),
    "ColorJitter" : transforms.ColorJitter(brightness=0.9, contrast=0.9)
}
#Convert the Tensor image back to a PIL image using a transform
pil_img = transforms.ToPILImage()(img)

#Plot a random application of each transform
f, axarr = plt.subplots(2,4)
for count, (name, t) in enumerate(sample_transforms.items()):
    row = count % 4
    col = count // 4
    axarr[col,row].imshow(t(pil_img), cmap='gray')
    axarr[col,row].set_title(name)
plt.show()

train_transform = transforms.Compose([
    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.98, 1.02)),
    transforms.ToTensor(),
])

test_transform = transforms.ToTensor()

mnist_train_t = torchvision.datasets.MNIST("./data", train=True, \
                                           transform=train_transform)
mnist_test_t = torchvision.datasets.MNIST("./data", train=False, \
                                          transform=test_transform)
mnist_train_loader_t = DataLoader(mnist_train_t, shuffle=True,  \
                                  batch_size=B, num_workers=5)
mnist_test_loader_t = DataLoader(mnist_test_t, batch_size=B, num_workers=5)

model_cnn_pool = nn.Sequential(
  nn.Conv2d(C, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),
  nn.Conv2d(filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),
  nn.Flatten(),
  nn.Linear(2*filters*D//(4**2), classes),
)
loss_func = nn.CrossEntropyLoss()
cnn_results_with_pool_augmented = train_simple_network(model_cnn_pool,\
                                     loss_func, \
                                     mnist_train_loader_t, \
                                     test_loader=mnist_test_loader_t, \
                                     score_funcs={'Accuracy': \
                                     accuracy_score}, \
                                     device=device, epochs=20)

sns.lineplot(x='epoch', y='test Accuracy', \
             data=cnn_results_with_pool, label='CNN w/ Max Pooling')
sns.lineplot(x='epoch', y='test Accuracy', \
             data=cnn_results_with_pool_augmented, \
             label='CNN w/ Max Pooling + Augmentation')


#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================

# -*- coding: utf-8 -*-
"""Week06_CNN_II.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LGILbXrpyRJDNpb0OF7Y-o5UUpaW3MK_
"""

# --- SETUP AND IMPORTS ---
from google.colab import drive
drive.mount('/content/drive')

import sys
# Add path to import custom modules (idlmam.py)
sys.path.append('/content/drive/My Drive/Colab Notebooks/Week06')

import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader
from tqdm.autonotebook import tqdm
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import pandas as pd
from sklearn.metrics import accuracy_score
import time
from idlmam import *

import torchvision
from torchvision import transforms
from scipy.signal import convolve

# --- LOADING MNIST DATA ---
# Load training and test data.
# transform=transforms.ToTensor() converts images to PyTorch tensors (C, H, W)
mnist_data_train = torchvision.datasets.MNIST("./data", train=True,\
                                              download=True, \
                                              transform=transforms.ToTensor())

mnist_data_test = torchvision.datasets.MNIST("./data", train=False,\
                                   download=True, \
                                   transform=transforms.ToTensor())

# Access a single example
x_example, y_example = mnist_data_train[0]

# --- MANUAL DATA AUGMENTATION (SHIFTING) ---
# Select the first image
img_indx = 0
img, correct_class = mnist_data_train[img_indx]
img = img[0,:] # Extract the 2D image (remove channel dim)

# Create shifted versions of the image using numpy.roll
# np.roll shifts elements along a specified axis.
# Shift right by 1 pixel (axis 1), then down by 1 pixel (axis 0)
img_lr = np.roll(np.roll(img, 1, axis=1), 1, axis=0) 
# Shift left by 1 pixel (axis 1), then up by 1 pixel (axis 0)
img_ul = np.roll(np.roll(img, -1, axis=1), -1, axis=0) 

# Visualize the original and shifted images
f, axarr = plt.subplots(1,3) 
axarr[0].imshow(img, cmap='gray')
axarr[1].imshow(img_lr, cmap='gray')
axarr[2].imshow(img_ul, cmap='gray')
plt.show()

# --- DEVICE SETUP ---
if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

# --- HYPERPARAMETERS ---
B = 32 # Batch size
mnist_train_loader = DataLoader(mnist_data_train, batch_size=B, shuffle=True)
mnist_test_loader = DataLoader(mnist_data_test, batch_size=B)

D = 28*28 # Input dimensions (flattened)
C = 1     # Channels
classes = 10 # Output classes
filters = 16 # Number of filters
K = 3     # Kernel size

# --- BASELINE LINEAR MODEL (MLP) ---
model_linear = nn.Sequential(
  nn.Flatten(), 
  nn.Linear(D, 256),
  nn.Tanh(),
  nn.Linear(256, classes),
)

# --- SIMPLE CNN MODEL ---
model_cnn = nn.Sequential(
  nn.Conv2d(C, filters, K, padding=K//2), 
  nn.Tanh(),
  nn.Flatten(),
  nn.Linear(filters*D, classes),
)

loss_func = nn.CrossEntropyLoss()

# Train the simple CNN
cnn_results = train_simple_network(model_cnn, loss_func,\
                       mnist_train_loader, test_loader=mnist_test_loader,\
                         score_funcs={'Accuracy': accuracy_score}, \
                         device=device, epochs=20)

# --- EVALUATING ROBUSTNESS ---
# Set model to evaluation mode (important for inference)
model = model_cnn.cpu().eval()

# Helper function for prediction
def pred(model, img):
    with torch.no_grad(): # Disable gradient calculation
        w, h = img.shape 
        # Convert to tensor if numpy array
        if not isinstance(img, torch.Tensor):
            img = torch.tensor(img) 

        # Reshape to (Batch, Channel, Height, Width) -> (1, 1, 28, 28)
        x = img.reshape(1,-1,w,h)
        logits = model(x) 
        y_hat = F.softmax(logits, dim=1) # Convert to probabilities
    return y_hat.numpy().flatten() 

# Predict on original and shifted images
img_pred = pred(model, img)
img_lr_pred = pred(model, img_lr)
img_ul_pred = pred(model, img_ul)

# Print probabilities for the correct class
# Notice how shifting the image significantly drops the confidence!
print("Org Img Class {} Prob:          ".format(correct_class) , \
      img_pred[correct_class])
print("Lower Right Img Class {} Prob: ".format(correct_class) , \
      img_lr_pred[correct_class])
print("Upper Left Img Class {} Prob:   ".format(correct_class) , \
      img_ul_pred[correct_class])


# --- CNN WITH MAX POOLING ---
# Max Pooling makes the network more robust to small translations (shifts).
# It also reduces the spatial dimensions, reducing computation.

model_cnn_pool = nn.Sequential(
  # Block 1
  nn.Conv2d(C, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2), # Pool 1: Reduces size by factor of 2 (28 -> 14)

  # Block 2: Increase filters (common practice when size decreases)
  nn.Conv2d(filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2), # Pool 2: Reduces size by factor of 2 (14 -> 7)

  nn.Flatten(),
  
  # Calculate Linear input size:
  # Filters doubled (2*filters).
  # Image width/height halved twice (D // 4^2). D=28*28, so 28/4 = 7. 7*7=49.
  nn.Linear(2*filters*D//(4**2), classes),
)

loss_func = nn.CrossEntropyLoss()

# Train CNN with Pooling
cnn_results_with_pool = train_simple_network(model_cnn_pool, loss_func, \
                                             mnist_train_loader, \
                                             test_loader=mnist_test_loader, \
                                             score_funcs={'Accuracy': accuracy_score}, \
                                             device=device, epochs=20)

# Evaluate robustness again with the pooled model
model = model_cnn_pool.cpu().eval()
img_pred = pred(model, img)
img_lr_pred = pred(model, img_lr)
img_ul_pred = pred(model, img_ul)

# Notice improved robustness (probabilities should be higher/more consistent)
print("Org Img Class {} Prob:          ".format(correct_class) , img_pred[correct_class])
print("Lower Right Img Class {} Prob: ".format(correct_class) , img_lr_pred[correct_class])
print("Uper Left Img Class {} Prob:    ".format(correct_class) , img_ul_pred[correct_class])

# Plot comparison
sns.lineplot(x='epoch', y='test Accuracy', data=cnn_results, label='Simple CNN')
sns.lineplot(x='epoch', y='test Accuracy', data=cnn_results_with_pool, label='CNN w/ Max Pooling')

# --- DATA AUGMENTATION ---
# Define a dictionary of common transforms for visualization
sample_transforms = {
    "Rotation" : transforms.RandomAffine(degrees=45),
    "Translation" : transforms.RandomAffine(degrees=0, translate=(0.1,0.1)),
    "Shear": transforms.RandomAffine(degrees=0, shear=45),
    "RandomCrop" : transforms.RandomCrop((20,20)),
    "Horizontal Flip" : transforms.RandomHorizontalFlip(p=1.0),
    "Vertical Flip": transforms.RandomVerticalFlip(p=1.0),
    "Perspective": transforms.RandomPerspective(p=1.0),
    "ColorJitter" : transforms.ColorJitter(brightness=0.9, contrast=0.9)
}

# Convert tensor back to PIL Image for visualization (transforms work on PIL)
pil_img = transforms.ToPILImage()(img)

# Visualize transforms
f, axarr = plt.subplots(2,4)
for count, (name, t) in enumerate(sample_transforms.items()):
    row = count % 4
    col = count // 4
    axarr[col,row].imshow(t(pil_img), cmap='gray')
    axarr[col,row].set_title(name)
plt.show()

# --- TRAINING WITH AUGMENTATION ---
# Compose multiple transforms for the training set.
# This generates new variations of images on the fly during training.
train_transform = transforms.Compose([
    transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.98, 1.02)),
    transforms.ToTensor(), # Always end with ToTensor
])

test_transform = transforms.ToTensor() # No augmentation for test set!

# Reload datasets with transforms
mnist_train_t = torchvision.datasets.MNIST("./data", train=True, \
                                           transform=train_transform)
mnist_test_t = torchvision.datasets.MNIST("./data", train=False, \
                                          transform=test_transform)

# DataLoaders with num_workers > 0 for parallel loading
mnist_train_loader_t = DataLoader(mnist_train_t, shuffle=True,  \
                                  batch_size=B, num_workers=5)
mnist_test_loader_t = DataLoader(mnist_test_t, batch_size=B, num_workers=5)

# Re-initialize the pooled CNN model
model_cnn_pool = nn.Sequential(
  nn.Conv2d(C, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(filters, filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),
  nn.Conv2d(filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.Conv2d(2*filters, 2*filters, 3, padding=3//2),
  nn.Tanh(),
  nn.MaxPool2d(2),
  nn.Flatten(),
  nn.Linear(2*filters*D//(4**2), classes),
)

loss_func = nn.CrossEntropyLoss()

# Train with augmented data
cnn_results_with_pool_augmented = train_simple_network(model_cnn_pool,\
                                     loss_func, \
                                     mnist_train_loader_t, \
                                     test_loader=mnist_test_loader_t, \
                                     score_funcs={'Accuracy': \
                                     accuracy_score}, \
                                     device=device, epochs=20)

# Compare results: Augmentation usually leads to better generalization (higher test accuracy)
sns.lineplot(x='epoch', y='test Accuracy', \
             data=cnn_results_with_pool, label='CNN w/ Max Pooling')
sns.lineplot(x='epoch', y='test Accuracy', \
             data=cnn_results_with_pool_augmented, \
             label='CNN w/ Max Pooling + Augmentation')