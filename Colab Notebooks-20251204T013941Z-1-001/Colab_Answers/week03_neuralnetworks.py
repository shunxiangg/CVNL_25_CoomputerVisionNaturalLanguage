# -*- coding: utf-8 -*-
"""Week03_NeuralNetworks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DuUuKlpYV0HLgOWjKGrQN-S4ilxqkAyY
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import *
from idlmam import *

def train_simple_network(model, loss_func, training_loader, \
                         epochs=20, device="cpu"):
  #Yellow step is done here. Creates the optimizer and moves the model
  #to compute device. SGD in the stochastic descent over the parameters
  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

  #places the model on the correct compute resource
  model.to(device)

  #The nested loop handle the red steps, iterating through all data (batches)
  #multiple times (epochs)
  for epoch in tqdm(range(epochs), desc="Epoch"):

    model = model.train() #Puts our model in training mode
    running_loss = 0.0

    for inputs, labels in tqdm(training_loader, desc="Batch", leave=False):
      #Moves the batch of data to the device we are using.
      #This is the last red step.
      inputs = moveTo(inputs, device)
      labels = moveTo(labels, device)

      #First a yellow step: prepare the optimizer. Most PyTorch code
      #does this first to make sure everything is in a clean and
      #ready state. PyTorch stores gradients in a mutable data
      #structure, so we need to set it to a clean state before we use it.
      #Otherwise, it will have old information from a previous iteration.
      optimizer.zero_grad()

      #This line and the next perform the two blue steps.
      y_hat = model(inputs)

      loss = loss_func(y_hat, labels)

      #The remaining two yellow steps compute the gradient and
      #“.step()” the optimizer. The call on this line computes
      #the gradient due to the parameters.
      loss.backward()

      optimizer.step() # Updates all the parameters

      running_loss += loss.item()#grabs information that we would like to have

X = np.linspace(0, 20, num=200) #Creates one-dimensional input
y = X + np.sin(X)*2 + np.random.normal(size=X.shape) #Creates output
sns.scatterplot(x=X, y=y)

class Simple1DRegressionDataset(Dataset):
  def __init__(self, X, y):
    super(Simple1DRegressionDataset, self).__init__()
    self.X = X.reshape(-1,1)
    self.y = y.reshape(-1,1)
  def __getitem__(self, index):
    return torch.tensor(self.X[index,:], dtype=torch.float32),\
    torch.tensor(self.y[index], dtype=torch.float32)
  def __len__(self):
    return self.X.shape[0]

training_loader = DataLoader(Simple1DRegressionDataset(X, y), shuffle=True)

in_features = 1
out_features = 1
model = nn.Linear(in_features, out_features)
loss_func = nn.MSELoss()
device = torch.device("cuda")
train_simple_network(model, loss_func, training_loader, device=device)

with torch.no_grad():
  Y_pred = model(torch.tensor(X.reshape(-1,1), device=device,\
                              dtype=torch.float32)).cpu().numpy()

#The data
sns.scatterplot(x=X, y=y, color='blue', label='Data')
#What the model learnt
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Linear Model')

model = nn.Sequential( #Input “layer” is implicitly the input.
nn.Linear(1, 10), #Hidden layer
nn.Linear(10, 1), #Output layer
)

train_simple_network(model, loss_func, training_loader)

with torch.no_grad():
  #Shape of (N, 1)
  Y_pred = model(torch.tensor(X.reshape(-1,1), \
                              dtype=torch.float32)).cpu().numpy()

#The data
sns.scatterplot(x=X, y=y, color='blue', label='Data')
#What our model learned
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model')

activation_input = np.linspace(-2, 2, num=200)

tanh_activation = np.tanh(activation_input)
sigmoid_activation = np.exp(activation_input)/(np.exp(activation_input)+1)

sns.lineplot(x=activation_input, y=activation_input, \
             color='black', label="linear")
sns.lineplot(x=activation_input, y=tanh_activation, \
             color='red', label="tanh(x)")
ax = sns.lineplot(x=activation_input, y=sigmoid_activation, \
                  color='blue', label="$σ(x)$")

model = nn.Sequential(
  nn.Linear(1, 10), #Hidden layer
  nn.Tanh(), #Activation
  nn.Linear(10, 1), #Output layer
)
train_simple_network(model, loss_func, training_loader, epochs=200)


with torch.no_grad():
  Y_pred = model(torch.tensor(X.reshape(-1,1),\
                              dtype=torch.float32)).cpu().numpy()
#The data
sns.scatterplot(x=X, y=y, color='blue', label='Data')
#What our model learned
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model')

from sklearn.datasets import make_moons

X, y = make_moons(n_samples=200, noise=0.05)

sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y)

classification_dataset = \
torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), \
                               torch.tensor(y, dtype=torch.long))

training_loader = DataLoader(classification_dataset)

in_features = 2
out_features = 2
model = nn.Linear(in_features, out_features)

loss_func = nn.CrossEntropyLoss()
train_simple_network(model, loss_func, training_loader, epochs=50)

def visualize2DSoftmax(X, y, model, title=None):
  x_min = np.min(X[:,0])-0.5
  x_max = np.max(X[:,0])+0.5
  y_min = np.min(X[:,1])-0.5
  y_max = np.max(X[:,1])+0.5

  xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20),\
                       np.linspace(y_min, y_max, num=20), indexing='ij')
  xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))

  with torch.no_grad():
    logits = model(torch.tensor(xy_v, dtype=torch.float32))
    y_hat = F.softmax(logits, dim=1).numpy()

  cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20),\
                    levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)
  ax = plt.gca()
  sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=ax)

  if title is not None:
    ax.set_title(title)

visualize2DSoftmax(X, y, model)

model = nn.Sequential(
nn.Linear(2, 30),
nn.Tanh(),
nn.Linear(30, 30),
nn.Tanh(),
nn.Linear(30, 2),
)

train_simple_network(model, loss_func, training_loader, epochs=250)

visualize2DSoftmax(X, y, model)



#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================
#=========================================================

# -*- coding: utf-8 -*-
"""Week03_NeuralNetworks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DuUuKlpYV0HLgOWjKGrQN-S4ilxqkAyY
"""

# Standard setup for Google Colab environment
from google.colab import drive
drive.mount('/content/drive')

import sys
# Adding the path to where 'idlmam.py' is located so we can import from it
sys.path.append('/content/drive/My Drive/Colab Notebooks')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import *
from idlmam import * # Contains helper functions like moveTo()

# --- TRAINING LOOP FUNCTION ---
# This is a reusable function to train a PyTorch model.
# It encapsulates the standard optimization loop: Forward -> Loss -> Backward -> Update
def train_simple_network(model, loss_func, training_loader, \
                         epochs=20, device="cpu"):
  # 1. Setup Optimizer
  # Using SGD (Stochastic Gradient Descent). 'lr' is learning rate (step size).
  # model.parameters() gives the optimizer access to all trainable weights in the model.
  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)

  # 2. Move Model to Device
  # Sends the model parameters to GPU (cuda) or CPU.
  model.to(device)

  # 3. Epoch Loop (iterates through the entire dataset multiple times)
  for epoch in tqdm(range(epochs), desc="Epoch"):

    model = model.train() # Set model to training mode (enables Dropout/BatchNorm if present)
    running_loss = 0.0

    # 4. Batch Loop (iterates through the dataset in chunks)
    for inputs, labels in tqdm(training_loader, desc="Batch", leave=False):
      # Move data to the same device as the model (critical for GPU training)
      inputs = moveTo(inputs, device)
      labels = moveTo(labels, device)

      # 5. Zero Gradients
      # PyTorch accumulates gradients by default. We must clear them before each update step.
      optimizer.zero_grad()

      # 6. Forward Pass
      # Pass inputs through the model to get predictions (y_hat)
      y_hat = model(inputs)

      # 7. Compute Loss
      # Compare predictions with ground truth labels
      loss = loss_func(y_hat, labels)

      # 8. Backward Pass (Backpropagation)
      # Computes gradients (dLoss/dWeights) for all parameters
      loss.backward()

      # 9. Optimizer Step
      # Updates parameters using the gradients computed above (Weights -= lr * gradient)
      optimizer.step() 

      running_loss += loss.item() # Track total loss for statistics

# --- DATA GENERATION (Synthetic Regression) ---
import numpy as np
import seaborn as sns
# Create simple 1D data: y = x + sin(x)*2 + noise
X = np.linspace(0, 20, num=200) 
y = X + np.sin(X)*2 + np.random.normal(size=X.shape) 
sns.scatterplot(x=X, y=y) # Visualize

# --- CUSTOM DATASET FOR REGRESSION ---
class Simple1DRegressionDataset(Dataset):
  def __init__(self, X, y):
    super(Simple1DRegressionDataset, self).__init__()
    # Reshape to (N, 1) because PyTorch Linear layers expect 2D tensors [Batch, Features]
    self.X = X.reshape(-1,1)
    self.y = y.reshape(-1,1)
  
  def __getitem__(self, index):
    # Returns a single pair of (input, target) as float tensors
    return torch.tensor(self.X[index,:], dtype=torch.float32),\
    torch.tensor(self.y[index], dtype=torch.float32)
  
  def __len__(self):
    return self.X.shape[0]

# Create DataLoader (handles batching and shuffling)
training_loader = DataLoader(Simple1DRegressionDataset(X, y), shuffle=True)

# --- EXPERIMENT 1: LINEAR REGRESSION ---
in_features = 1
out_features = 1
# A single Linear layer is mathematically equivalent to Linear Regression
model = nn.Linear(in_features, out_features) 

loss_func = nn.MSELoss() # Mean Squared Error is standard for Regression
device = torch.device("cuda")

# Run training
train_simple_network(model, loss_func, training_loader, device=device)

# --- VISUALIZATION 1 ---
# Inference (Prediction) without tracking gradients
with torch.no_grad():
  Y_pred = model(torch.tensor(X.reshape(-1,1), device=device,\
                              dtype=torch.float32)).cpu().numpy()

# Plot results
sns.scatterplot(x=X, y=y, color='blue', label='Data')
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Linear Model')
# Result: A straight line that doesn't fit the sin wave curve well (Underfitting)

# --- EXPERIMENT 2: STACKED LINEAR LAYERS (Still Linear) ---
# Trying to improve by adding more layers, but WITHOUT activation functions.
model = nn.Sequential( 
  nn.Linear(1, 10), # Hidden layer (size 10)
  nn.Linear(10, 1), # Output layer
)

train_simple_network(model, loss_func, training_loader)

with torch.no_grad():
  Y_pred = model(torch.tensor(X.reshape(-1,1), \
                              dtype=torch.float32)).cpu().numpy()

# Result: Still a straight line. Stacking Linear layers without non-linearities 
# collapses into a single Linear layer.
sns.scatterplot(x=X, y=y, color='blue', label='Data')
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model')

# --- ACTIVATION FUNCTIONS DEMO ---
activation_input = np.linspace(-2, 2, num=200)
tanh_activation = np.tanh(activation_input)
# Sigmoid: 1 / (1 + e^-x)
sigmoid_activation = np.exp(activation_input)/(np.exp(activation_input)+1)

# Visualization of how activation functions bend the line
sns.lineplot(x=activation_input, y=activation_input, \
             color='black', label="linear")
sns.lineplot(x=activation_input, y=tanh_activation, \
             color='red', label="tanh(x)")
ax = sns.lineplot(x=activation_input, y=sigmoid_activation, \
                  color='blue', label="$σ(x)$")

# --- EXPERIMENT 3: NON-LINEAR NEURAL NETWORK ---
# Now we add Tanh() activation between layers.
model = nn.Sequential(
  nn.Linear(1, 10), # Hidden layer
  nn.Tanh(),        # Activation (Non-linearity)
  nn.Linear(10, 1), # Output layer
)
train_simple_network(model, loss_func, training_loader, epochs=200)

with torch.no_grad():
  Y_pred = model(torch.tensor(X.reshape(-1,1),\
                              dtype=torch.float32)).cpu().numpy()

# Result: The model can now fit the curve!
sns.scatterplot(x=X, y=y, color='blue', label='Data')
sns.lineplot(x=X, y=Y_pred.ravel(), color='red', label='Model')

# --- EXPERIMENT 4: CLASSIFICATION ---
from sklearn.datasets import make_moons

# Create classification data (2 interleaved half circles)
X, y = make_moons(n_samples=200, noise=0.05)
sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y)

# Dataset for classification
classification_dataset = \
torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), \
                               torch.tensor(y, dtype=torch.long)) # Labels must be Long (Integers)

training_loader = DataLoader(classification_dataset)

# Simple Linear Model for Classification
in_features = 2
out_features = 2 # 2 Output neurons for 2 classes (Binary classification)
model = nn.Linear(in_features, out_features)

# CrossEntropyLoss is standard for classification (combines Softmax + Log Loss)
loss_func = nn.CrossEntropyLoss()
train_simple_network(model, loss_func, training_loader, epochs=50)

# --- VISUALIZING DECISION BOUNDARY ---
def visualize2DSoftmax(X, y, model, title=None):
  # Create a grid of points covering the data range
  x_min = np.min(X[:,0])-0.5
  x_max = np.max(X[:,0])+0.5
  y_min = np.min(X[:,1])-0.5
  y_max = np.max(X[:,1])+0.5

  xv, yv = np.meshgrid(np.linspace(x_min, x_max, num=20),\
                       np.linspace(y_min, y_max, num=20), indexing='ij')
  xy_v = np.hstack((xv.reshape(-1,1), yv.reshape(-1,1)))

  # Get model predictions for every point on the grid
  with torch.no_grad():
    logits = model(torch.tensor(xy_v, dtype=torch.float32))
    y_hat = F.softmax(logits, dim=1).numpy()

  # Plot contour map of decision boundary
  cs = plt.contourf(xv, yv, y_hat[:,0].reshape(20,20),\
                    levels=np.linspace(0,1,num=20), cmap=plt.cm.RdYlBu)
  ax = plt.gca()
  sns.scatterplot(x=X[:,0], y=X[:,1], hue=y, style=y, ax=ax)

  if title is not None:
    ax.set_title(title)

# Result: Linear decision boundary (straight line separation) - suboptimal
visualize2DSoftmax(X, y, model)

# --- EXPERIMENT 5: DEEP NEURAL NETWORK FOR CLASSIFICATION ---
# More layers + Tanh activation allows for complex, curved decision boundaries
model = nn.Sequential(
  nn.Linear(2, 30),
  nn.Tanh(),
  nn.Linear(30, 30),
  nn.Tanh(),
  nn.Linear(30, 2),
)

train_simple_network(model, loss_func, training_loader, epochs=250)

# Result: Curved decision boundary that fits the "moons" shape well
visualize2DSoftmax(X, y, model)